목차
9.1최대마진분류기(marximal margin classifier) – 클래스들이 선형 경계에 의해 구별될 수 있어야 한다.
9.2 서포트벡터분류기(support vector classifier) -  최대마진분류기 확장(선형)
9.3 서포트벡터머신 – 비선형의 클래스 경계를 수용(두개의 클래스가 있는2진 분류)
9.4 서포트벡터머신 – 클래스 수가 3개 이상인 경우로 확장
9.5 서포트벡터머신과 로지스틱회귀와 같은 다른 통계 방법사이의 밀접한 관련성



9.1최대마진분류기(marximal margin classifier) – 클래스들이 선형 경계에 의해 구별될 수 있어야 한다.

9.1.1 초평면은 무엇인가?
P차원 공간에서 초평면은 차원이 p-1인 평평한 아핀 부분공간
(2차원)
Ax1+bx2+c=0
Ax1+bx2+c>0
Ax1+bx2+c<0

9.1.2 분리초평면을 사용한 분류
훈련데이터를 기반으로 분류기를 개발하고 싶다!
분류 : 선형판별분석, 로지스틱회귀, 분류트리, 배깅, 부스팅
분리 초평면의 개념에 기초한 분류기의 새로운 기법
yi=1 Ax1+bx2+c>0
yi=-1 Ax1+bx2+c<0
즉, yi(Ax1+bx2+c)>0을 모두 만족한다.
f(x)= Ax1+bx2+c가 양수이면 클래스1, f(x)의 절대값이 크면 초평면으로부터 멀리 떨어져 있다. (=클래스할당에 확신할 수 있다)
분리 초평면에 기초한 분류기는 선형결정경계로 이루어진다.

9.1.3 최대마진분류기
초평면을 사용하여 데이터가 완벽하게 분리된다면 회전가능하기 때문에 무한개의 초평면이 존재한다. 
그렇다면 이러한 무한개의 초평면중에서 어떤 초평면을 사용해야 하는가에 대한 합리적인 선택법이 필요하다.
그렇다면 훈련 관측치들로부터 가장 멀리 떨어진 분리 초평면인 최대마진초평면을 선택하는 것이 자연스럽다!
마진 : 관측치들에서 초평면까지의 가장 짧은 거리(수직)
즉, 최대마진초평면은 초평면 중에서도 마진이 가장 작은 것이다.
최대 마진 분류기는 보통은 성공적으로 동작하지만 p값이 클 때 과적합에 이를 수 있다.
최대 마진 초평면은 두 클래스 사이에 끼울 수 있는 가장 넓은 평판의 중간선을 나타낸다!
마진에 위치한 세 관측치는 서포트 벡터로 알려져있다.
그 이유는 이 점들이 이동하면 최대 마진 초평면도 이동될 것이라는 의미에서 최대마진초평면을 서포트 하기 떄문이다.
흥미롭게도 최대마진초평면은 서포트벡터에는 직간접적으로 의존적이지만 다른 관측치들에는 의존적이지 않다.
즉, 다른 어떤 관측치의 이동도 이러한 이동으로 인해 그 관측치가 마진에 의해 설정된 경계를 넘어가지 않으면 분리 초평면에 영향을 주지 않는다.
이러한 성질은 정말 중요하다!!! 서포트 벡터분류기와 서포트벡터머신을 다룰 떄 살펴볼것이다. M은 마진을 나타낸다.

9.1.4 최대마진분류기의 구성
책 참고

9.1.5분류 불가능한 경우
많은 경우에 분리 초평면이 존재하지 않는다.
이 경우에는 도 클래스를 정확하게 분류할 수 없다.
분리초평면의 개념을 확장하여 소프트 마진이라는 것을 사용하여 클래스들을 거의 분류하는 초평면을 개발할 수 있다.
최대마진 분류기를 분류할 수 없는 경우로 일반화한 것은 서포트 벡터 분류기로 알려져 있다.

9.2 서포트벡터분류기(support vector classifier) -  최대마진분류기 확장(선형)

9.2.1 서포트벡터분류기의 개요
최대마진초평면이 단일 관측치변화에 극도로 민감하다!- 훈련데이터 과적합가능성

개별관측치에 대해 더 로버스트하다
대부분의 훈련관측치들을 더 잘 분류한다
서포트벡터분류기(=서프트마진분류기)
일부관측치들은 마진의 옳지 않은 쪽에 있거나 심지어는 초평면의 옳지 않은 쪽에 있을수 있도록 허용된다.
마진이 소프트하다고 하는 이유는 일부 훈련 관측치들에 의해 마진이 위반되기 때문이다.

9.2.2 서포트벡터분류기의 세부사항
C가 증가함에 따라 마진 위반에 대한 허용 정도가 더 크게 되어 마진의 폭이 넓어질 것이다.
Scikitlearn에서 C가 줄어드니까 허용범위가 크게되고 마진의 폭은 줄어드나?

분산:테스트셋,편향:훈련셋

마진상에 직접 놓으거나 클래스에 대한 마진의 옳지 않은 쪽에 있는 관측치들은 서포트벡터들로 알려져 있다.

서포트벡터분류기의 결정규칙이 잠재적으로 작은 수의 훈련 관측치들에 기반을 두고 있다는 사실은 초평면에서 멀리 떨어진 관측치들의 행동에 상당히 로버스트하다는 것을 의미한다. 이 성질은 이전의 장들에서 보았던 몇몇 다른 분류방법들과 다른 점이다.

LDA분류규칙은 각 클래스내의 모든 관측치들의 평균과 모든 관측치들을 사용하여 계산된 클래스 내 공분산행렬에 의존한다.
이에 반해 로지스틱 회귀는 LDA와는 달리 결정경계에서 멀리 떨어진 관측치들에 대한 민감도가 매우 낮다. 그래서 서포트벡터분류기와 로지스틱회귀는 밀접하게 관련이 있다는 것을 9.5에서 볼 것이다.

9.3 서포트벡터머신 – 비선형의 클래스 경계를 수용(두개의 클래스가 있는2진 분류)

9.3.1 비선형 결정경계를 가진 분류
p개의 변수를 2p개의 변수로 늘려서 사용 제곱텀 포함

9.3.2 서포트 벡터 머신
서포트벡터머신은 서포트벡터분류기의 확장이다.
커널을 사용하여 특정한 방식으로 변수공간을 확장한 결과이다.
Example – python library를 활용한 데이터분석 활용

9.3.3 심장질환 자료에 적용



   
[참고]파이썬라이브러리를활용한데이터분석


9.4 서포트벡터머신 – 클래스 수가 3개 이상인 경우로 확장(클래스가 2개보다 많은 SVM)

9.4.1 일대일 분류
kC2개를 뽑아서 두개를 비교하여 각각의 K클래스에 할당된 횟수를 기록한다. 이 검정 관측치를 kC2개의 쌍별분류에서 가장 자주 할당된 클래스에 할당함으로써 분류가 완료된다.

두번째 멀티클래스 분류방법인데 
m개의 클래스 중 2개를 선택하여 2클래스에 대한 결정 초평면을 만든다. 그렇게 되면 결정초평면과 분류기는 mC2개만큼 생기게 된다.M(M-1)/2 개 된다. 
이제 test 할때는 투표개념을 도입하여 분류하는데 새로들어온 sample x에 대해서 초평면 dij(x)가 x를 class i로 분류하면 class i에 +1점, j로 분류하면 class j에 +1 점을 준다. 즉 mC2개의 분류기가 M개의 class에 대해서 투표를 하는 것이다.  이렇게 모든 초평면에 대해서 분류하였을때 가장 높은 점수를 획득한 class가 predicted된 클래스이다. 
이때 얻을 수 있는 최대 표값은 M-1개 이고 이방법은 one vs rest가 가지고 있는 문제를 가지진 않는다. 

그러나, 클래스의 개수 M이 커지면 이진 분류기의 수가 많아지고 결국 learning 과 test에 걸리는 시간이 많아진다.

실제적으로 가장 많이 사용되는 분류방법은 2가지 단점에도 불구 하고  one vs rest 방법이다. 이진 svm을 멀티클래스 svm으로 확장하는 방법에 대한 연구도 많이 논의 중이다. 


출처: http://ddiri01.tistory.com/207 [처음의 마음]


9.4.2 일대전부(one-versus-all) 분류
전체 class의 수가 M개 라고 하면 i 번째 class의 부류 와 i클래스를 제외한 나머지 M-1개의 클래스가 속하는 클래스 로 이진화 시키고 분류기를 만들고, 이와 같은 작업을 M번 만큼 한다. 그러면 총 M 개의 이진분류기가 만들어 지게 된다.  즉 i class에 속하는 샘플을 +1 라벨을 붙이고 나머지 샘플에 -1라벨을 붙인다. 그래서 훈련집합(training set)을 만들게 된다.  svm의 결정 초평면도 M개가 만들어 지는데 j  번째 초평면을 d(j)라 할 수 있다. 
실제 Test를 할때에는 M개의 초평면에 모두 test를 하게 되는데 m번의 분류에서 1가지만 양수를 출력하고 나머지는 모두 음수를 출력한다면 문제될것이 전혀 없지만, 항상 그렇게 된다는 보장은 없다 .
따라서, m번의 test중 가장 큰 d(j)를 갖는것을 예측된 class로 한다.


출처: http://ddiri01.tistory.com/207 [처음의 마음]

9.5 서포트벡터머신과 로지스틱회귀와 같은 다른 통계 방법사이의 밀접한 관련성(로지스틱 회귀에 대한 상관관계)
C랑 람다랑 비슷한 역할
람다가 커지면 허용수준이 높아져서 beta값들은 줄어든다. 분산(테스트셋)은 낮지만 편향(훈련셋)이 높은 분류기가 만들어진다.
손실 + 페널티 형태를 취한다.
두 손실함수는 유사하게 동작한다.
로지스틱회귀는 손실함수가 어디에서도 정확하게 영이 되지 않는다.
그러나 서포트벡터머신의 손실함수는 마진의 올바른 쪽에 있는 관측치들에서 영의 값을 가진다.
그래서 클래스들이 잘 분리되어 있는 경우에는 SVM이 더 낫고, 겹치는 경우에는 로지스틱회귀가 선호된다.
조율파라미터 C, 람다는 매우 중요하게 생각된다.(과소적합, 과대적합정도는 나타냄)


9.6 Lab: 서포트 벡터 머신
R의 e1071 라이브러리를 사용하여 서포트 벡터 분류기와 SVM을 살펴보자
또는 LiblineaR 라이브러리를 사용할 수 있는데 이것은 아주 큰 선형 문제에 유용하다.


